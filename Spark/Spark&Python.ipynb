{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d277ed",
   "metadata": {},
   "source": [
    "# Spark using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bf8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74144396",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc81950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565d2e2",
   "metadata": {},
   "source": [
    "## create the spark session and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c7b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ec69d",
   "metadata": {},
   "source": [
    "## Initialize Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b7853",
   "metadata": {},
   "source": [
    "## RDDs : Resilient Distributed Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa47526",
   "metadata": {},
   "source": [
    "### Create an RDD : which has integers from 1 to 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd2d69",
   "metadata": {},
   "source": [
    "Create the RDD by \"sc.parallelize()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc8747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = range(1,30)\n",
    "xRDD = sc.parallelize(data,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e35c8",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f34e0",
   "metadata": {},
   "source": [
    "Reduce each element in the RDD by 1, another one is filter x<10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862e9f2",
   "metadata": {},
   "source": [
    "lambda is an inline function in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "newRDD1 = xRDD.map(lambda x: x-1)\n",
    "filteredRDD = newRDD.filter(lambda x : x<10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad90fd",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filteredRDD.collect()) ##print all data inside\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358eb7a8",
   "metadata": {},
   "source": [
    "## DataFrames and SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5afc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"filename.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe as well as the data schema\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c405268",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SQL\n",
    "spark.sql(\"SELECT name FROM people\").show()\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
